name: AI QA Agent - CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_url:
        description: 'URL to test'
        required: false
        default: 'https://example.com'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # ====================
  # Code Quality Checks
  # ====================
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install black isort mypy pylint
          pip install -r requirements.txt
      
      - name: Run Black (code formatting)
        run: black --check modules/ worker/
      
      - name: Run isort (import sorting)
        run: isort --check-only modules/ worker/
      
      - name: Run mypy (type checking)
        run: mypy modules/ worker/ --ignore-missing-imports
        continue-on-error: true
      
      - name: Run pylint
        run: pylint modules/ worker/ --disable=C,R --exit-zero

  # ====================
  # Unit Tests
  # ====================
  test:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install pytest pytest-asyncio pytest-cov pytest-timeout
          pip install -r requirements.txt
      
      - name: Run unit tests
        run: |
          pytest tests/ \
            --cov=modules \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=junit/test-results.xml \
            --timeout=300
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.python-version }}
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: junit/test-results.xml

  # ====================
  # Integration Tests
  # ====================
  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Install Playwright
        run: |
          playwright install --with-deps chromium
      
      - name: Run integration tests
        env:
          REDIS_URL: redis://localhost:6379
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          pytest tests/integration/ \
            --timeout=600 \
            -v
      
      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: reports/

  # ====================
  # E2E Tests (Playwright)
  # ====================
  e2e:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: integration
    
    strategy:
      matrix:
        browser: [chromium, firefox, webkit]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install --with-deps ${{ matrix.browser }}
      
      - name: Run E2E tests
        env:
          TEST_URL: ${{ github.event.inputs.test_url || 'https://example.com' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m modules.spec_orchestrator \
            --requirement "Test $TEST_URL" \
            --browser ${{ matrix.browser }}
      
      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.browser }}
          path: |
            reports/
            generated_frameworks/
          retention-days: 30

  # ====================
  # Visual Regression Tests
  # ====================
  visual:
    name: Visual Regression Tests
    runs-on: ubuntu-latest
    needs: e2e
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install --with-deps chromium
      
      - name: Run visual regression tests
        env:
          PERCY_TOKEN: ${{ secrets.PERCY_TOKEN }}
        run: |
          python -m modules.visual_regression \
            --url https://example.com \
            --widths 1920,1280,768,375
      
      - name: Upload visual test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: visual-regression-results
          path: data/visual_diffs/

  # ====================
  # Docker Build & Push
  # ====================
  docker:
    name: Docker Build & Push
    runs-on: ubuntu-latest
    needs: [test, integration]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.DOCKER_USERNAME }}/ai-qa-agent
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./worker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ====================
  # Security Scan
  # ====================
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run Bandit security linter
        run: |
          pip install bandit
          bandit -r modules/ worker/ -f json -o bandit-report.json
        continue-on-error: true
      
      - name: Upload Bandit results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-results
          path: bandit-report.json

  # ====================
  # Deploy to Production
  # ====================
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [docker, e2e, visual]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production
      url: https://qa.example.com
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Deploy to Kubernetes
        uses: azure/k8s-deploy@v4
        with:
          namespace: ai-qa-production
          manifests: |
            k8s/deployment.yaml
            k8s/service.yaml
          images: |
            ${{ secrets.DOCKER_USERNAME }}/ai-qa-agent:${{ github.sha }}
      
      - name: Wait for deployment
        run: |
          kubectl rollout status deployment/ai-qa-worker -n ai-qa-production
      
      - name: Run smoke tests
        run: |
          curl -f https://qa.example.com/health/ready || exit 1

  # ====================
  # Performance Tests
  # ====================
  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: deploy
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Locust
        run: pip install locust
      
      - name: Run load tests
        run: |
          locust -f tests/performance/locustfile.py \
            --headless \
            --users 100 \
            --spawn-rate 10 \
            --run-time 5m \
            --host https://qa.example.com
      
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: reports/performance/
